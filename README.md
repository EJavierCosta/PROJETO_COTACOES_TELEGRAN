# Projeto de Pipeline de Dados e Chatbot de Cota√ß√µes com Delta Lake, Spark e Telegram
<div align="center">

![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.4.1-E25A1C?logo=apache-spark)
![Databricks](https://img.shields.io/badge/Databricks-Ready-FF3621?logo=databricks)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-2.4.0-00ADD4?logo=delta)
![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-CI/CD-2088FF?logo=github-actions)

</div>

## üìñ Sobre o Projeto
Este projeto demonstra a constru√ß√£o de um pipeline de dados robusto e uma API de consulta em linguagem natural para o mercado financeiro. A solu√ß√£o √© capaz de ingerir, processar e servir dados de cota√ß√µes de a√ß√µes atrav√©s de um chatbot no Telegram.

O n√∫cleo do projeto √© uma arquitetura Lakehouse constru√≠da sobre o **Delta Lake**, com um pipeline de dados totalmente gerenciado pelo **Apache Spark**, que processa informa√ß√µes em modo streaming e batch. Todo o ciclo de vida de desenvolvimento, deploy e agendamento de jobs √© automatizado atrav√©s de **CI/CD com GitHub Actions**.

## ‚úÖ Pr√©-requisitos
Antes de iniciar, garanta que voc√™ tenha acesso e as configura√ß√µes para as seguintes ferramentas:

- **Workspace Databricks:** Um ambiente Databricks configurado na AWS, Azure ou GCP.
- **Token de Acesso ao Databricks:** Um token de acesso pessoal para permitir que o GitHub Actions se autentique na API do Databricks.
- **Telegram Bot:** Um bot criado no Telegram com seu respectivo token de API.
- **Inst√¢ncia n8n:** Uma conta no n8n (Cloud ou auto-hospedada) para orquestrar o fluxo da API.
- **Conta no GitHub:** Para hospedar o reposit√≥rio e utilizar o GitHub Actions.

## üèõÔ∏è Arquitetura de Dados & Pipeline
A espinha dorsal do projeto √© uma arquitetura **Medallion** implementada com Delta Lake, que organiza os dados em camadas de qualidade crescente. Isso garante governan√ßa, confiabilidade e performance. O processamento entre as camadas √© orquestrado via notebooks Databricks utilizando Apache Spark.

```mermaid
graph TD
    subgraph "üì• Camada Transacional (Fonte de Dados)"
        A["API de Cota√ß√µes"]
    end

    subgraph "üèõÔ∏è Databricks Lakehouse"
        B["üíø Camada Bronze <br><i>(Transacional - Dados Brutos)</i>"]
        C["ü•à Camada Silver <br><i>(Transacional - Limpos e Validados)</i>"]
        D["ü•á Camada Gold <br><i>(Produ√ß√£o - Dados Agregados)</i>"]
        A -- Ingest√£o via Spark --> B
        B -- ETL via Spark --> C
        C -- Agrega√ß√£o via Spark --> D
    end

    subgraph "ü§ñ Camada de Consumo"
        E["Telegram Bot"]
        F["n8n Workflow"]
        G["Databricks Genie"]
        E -- Pergunta do Usu√°rio --> F
        F -- Conecta ao Genie --> G
        G -- Gera e Executa SQL --> D
        G -- Retorna Resultado --> F
        F -- Formata e Entrega Resposta --> E
    end

    subgraph "üîÑ Automa√ß√£o (CI/CD)"
        H["üíª Push na Branch 'main'"]
        I["üöÄ GitHub Actions"]
        J["‚öôÔ∏è Databricks Jobs"]
        H -- Dispara Gatilho --> I
        I -- Deploy/Update dos Jobs --> J
        J -- Agenda Job Bronze --> B
        J -- Agenda Job Silver --> C
        J -- Agenda Job Gold --> D
    end
```
### üì• Transactional Camada Transacional (Fonte de Dados)

- O ponto de partida de todo o pipeline s√£o as fontes de dados transacionais externas. Para este projeto, a fonte principal √© uma API financeira que fornece dados - do mercado de a√ß√µes.

- **API de Cota√ß√µes:** Atrav√©s de requisi√ß√µes a esta API, o pipeline busca dois tipos de informa√ß√£o:
  - **Dados Hist√≥ricos:** Uma carga inicial com o hist√≥rico de cota√ß√µes para um conjunto pr√©-definido de ativos.
  - **Cota√ß√µes Atuais:** Cargas recorrentes com os dados mais recentes das cota√ß√µes, permitindo que o Lakehouse se mantenha atualizado.
- **Ponto de Ingest√£o:** Este √© o processo que alimenta a camada Bronze, trazendo os dados brutos do ambiente externo para dentro do nosso ecossistema de dados para processamento.
  
### üíø Camada Bronze (Transacional - Dados Brutos)
-   **Prop√≥sito:** Ingest√£o de dados brutos de fontes externas (APIs de cota√ß√µes, arquivos, etc.). Esta camada funciona como um "data swamp" persistente, armazenando os dados em seu formato original.
-   **Tecnologia:** Tabelas em formato Delta, o que permite `ACID transactions`, versionamento de dados (`time travel`) e a capacidade de misturar cargas em batch e streaming.
-   **Processamento:** O Spark √© utilizado para conectar-se √†s fontes de dados e realizar a ingest√£o.

### ü•à Camada Silver (Transacional - Dados Limpos e Validados)
-   **Prop√≥sito:** Transformar os dados brutos da camada Bronze em informa√ß√µes limpas, validadas e enriquecidas. Aqui ocorrem processos de deduplica√ß√£o, tratamento de valores nulos, padroniza√ß√£o de tipos de dados e joins para enriquecimento.
-   **Tecnologia:** Tabelas Delta, aproveitando a performance de `MERGE` e `UPDATE` para manter a qualidade dos dados.
-   **Processamento:** Jobs do Spark (batch ou streaming) leem os novos dados que chegam na camada Bronze e aplicam as transforma√ß√µes.

### ü•á Camada Gold (Produ√ß√£o - Dados Agregados)
-   **Prop√≥sito:** Servir como a camada de consumo final, a "fonte da verdade" para os usu√°rios. As tabelas aqui s√£o agregadas e modeladas para fins espec√≠ficos de neg√≥cio.
-   **Tecnologia:** Tabelas Delta, frequentemente otimizadas com `Z-ORDER` para performance m√°xima de consulta.
-   **Exemplos de Tabelas:** `cotacoes_medias` (m√©dias de pre√ßo) e `cotacoes_variacoes` (varia√ß√£o percentual).
-   **Processamento:** Jobs do Spark em batch leem os dados da camada Silver, calculam as agrega√ß√µes complexas e atualizam as tabelas da camada Gold.

## üîÑ CI/CD com GitHub Actions: Infraestrutura como C√≥digo
Para garantir a integridade, automa√ß√£o e agilidade nas entregas, o projeto implementa um pipeline de CI/CD utilizando **GitHub Actions**.

-   **Automa√ß√£o Total:** O processo de deploy dos artefatos de c√≥digo (notebooks, scripts) e a configura√ß√£o dos jobs no Databricks s√£o totalmente automatizados.
-   **Gatilho:** Qualquer `push` ou `merge` na branch `main` dispara o workflow do GitHub Actions.
-   **Deploy e Agendamento via JSON:** A action se autentica no workspace Databricks e utiliza um arquivo de configura√ß√£o JSON presente no reposit√≥rio para criar ou atualizar os jobs. Este arquivo define qual notebook executar, em qual cluster, e seu respectivo agendamento (schedule).
-   **Benef√≠cios:** Esta abordagem de Infraestrutura como C√≥digo (IaC) garante que o ambiente de produ√ß√£o seja um reflexo fiel do que est√° versionado no Git, eliminando configura√ß√µes manuais, reduzindo erros e acelerando o ciclo de desenvolvimento.

## ü§ñ Camada de Consumo: Chatbot & API
A camada Gold, com seus dados de alta qualidade, serve como a base para a interface do usu√°rio.

-   **Databricks Genie:** Atua como uma camada de tradu√ß√£o de linguagem natural para SQL. Ele √© configurado para consultar apenas as tabelas da camada Gold.
-   **n8n:** Orquestra o fluxo da API, conectando o Telegram ao Databricks Genie, atuando como um intermedi√°rio leve entre o usu√°rio e o poder de processamento do Databricks.

## üöÄ Como Executar o Projeto
1.  **Setup do Ambiente:** Configure o workspace no Databricks (clusters, SQL Warehouse).
2.  **Configurar CI/CD:** Adicione os `secrets` necess√°rios (como o token de acesso ao Databricks e o host) nas configura√ß√µes do reposit√≥rio do GitHub para que o GitHub Actions possa se autenticar.
3.  **Definir os Jobs:** Edite o arquivo de configura√ß√£o `jobs.json` (ou nome similar) no reposit√≥rio para definir os notebooks a serem executados e seus respectivos agendamentos no formato `cron`.
4.  **Deploy:** Fa√ßa um push para a branch `main`. O GitHub Actions ser√° acionado e ir√° automaticamente criar ou atualizar os jobs no Databricks com o agendamento definido no JSON.
5.  **Configurar Camada de Consumo:** Configure o Databricks Genie para usar as tabelas da camada Gold e, em seguida, configure o workflow no n8n para conectar o Telegram ao Genie.

## üìú Licen√ßa
-  Este projeto est√° sob a licen√ßa MIT. Veja o arquivo LICENSE para mais detalhes.
  
## Resumo das Tecnologias
-   **Core do Pipeline:** Apache Spark (Structured Streaming e Batch)
-   **Armazenamento e Lakehouse:** Delta Lake
-   **Automa√ß√£o de CI/CD (IaC):** GitHub Actions
-   **Orquestra√ß√£o do Pipeline:** Databricks Jobs (gerenciados via API/JSON)
-   **Camada de IA/API:** Databricks Genie
-   **Interface do Usu√°rio:** Telegram
-   **Orquestra√ß√£o da API:** n8n
