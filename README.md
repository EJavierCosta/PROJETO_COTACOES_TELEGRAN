# Projeto de Pipeline de Dados e Chatbot de Cota√ß√µes com Delta Lake, Spark e Telegram

## üìñ Sobre o Projeto
Este projeto demonstra a constru√ß√£o de um pipeline de dados robusto e uma API de consulta em linguagem natural para o mercado financeiro. A solu√ß√£o √© capaz de ingerir, processar e servir dados de cota√ß√µes de a√ß√µes atrav√©s de um chatbot no Telegram.

O n√∫cleo do projeto √© uma arquitetura Lakehouse constru√≠da sobre o **Delta Lake**, com um pipeline de dados totalmente gerenciado pelo **Apache Spark**, que processa informa√ß√µes em modo streaming e batch. Todo o ciclo de vida de desenvolvimento, deploy e agendamento de jobs √© automatizado atrav√©s de **CI/CD com GitHub Actions**.

## üèõÔ∏è Arquitetura de Dados & Pipeline
A espinha dorsal do projeto √© uma arquitetura **Medallion** implementada com Delta Lake, que organiza os dados em camadas de qualidade crescente. Isso garante governan√ßa, confiabilidade e performance. O processamento entre as camadas √© orquestrado via notebooks Databricks utilizando Apache Spark.

### üíø Camada Bronze (Transacional - Dados Brutos)
-   **Prop√≥sito:** Ingest√£o de dados brutos de fontes externas (APIs de cota√ß√µes, arquivos, etc.). Esta camada funciona como um "data swamp" persistente, armazenando os dados em seu formato original.
-   **Tecnologia:** Tabelas em formato Delta, o que permite `ACID transactions`, versionamento de dados (`time travel`) e a capacidade de misturar cargas em batch e streaming.
-   **Processamento:** O Spark √© utilizado para conectar-se √†s fontes de dados e realizar a ingest√£o.

### ü•à Camada Silver (Transacional - Dados Limpos e Validados)
-   **Prop√≥sito:** Transformar os dados brutos da camada Bronze em informa√ß√µes limpas, validadas e enriquecidas. Aqui ocorrem processos de deduplica√ß√£o, tratamento de valores nulos, padroniza√ß√£o de tipos de dados e joins para enriquecimento.
-   **Tecnologia:** Tabelas Delta, aproveitando a performance de `MERGE` e `UPDATE` para manter a qualidade dos dados.
-   **Processamento:** Jobs do Spark (batch ou streaming) leem os novos dados que chegam na camada Bronze e aplicam as transforma√ß√µes.

### ü•á Camada Gold (Produ√ß√£o - Dados Agregados)
-   **Prop√≥sito:** Servir como a camada de consumo final, a "fonte da verdade" para os usu√°rios. As tabelas aqui s√£o agregadas e modeladas para fins espec√≠ficos de neg√≥cio.
-   **Tecnologia:** Tabelas Delta, frequentemente otimizadas com `Z-ORDER` para performance m√°xima de consulta.
-   **Exemplos de Tabelas:** `cotacoes_medias` (m√©dias de pre√ßo) e `cotacoes_variacoes` (varia√ß√£o percentual).
-   **Processamento:** Jobs do Spark em batch leem os dados da camada Silver, calculam as agrega√ß√µes complexas e atualizam as tabelas da camada Gold.

## üîÑ CI/CD com GitHub Actions: Infraestrutura como C√≥digo
Para garantir a integridade, automa√ß√£o e agilidade nas entregas, o projeto implementa um pipeline de CI/CD utilizando **GitHub Actions**.

-   **Automa√ß√£o Total:** O processo de deploy dos artefatos de c√≥digo (notebooks, scripts) e a configura√ß√£o dos jobs no Databricks s√£o totalmente automatizados.
-   **Gatilho:** Qualquer `push` ou `merge` na branch `main` dispara o workflow do GitHub Actions.
-   **Deploy e Agendamento via JSON:** A action se autentica no workspace Databricks e utiliza um arquivo de configura√ß√£o JSON presente no reposit√≥rio para criar ou atualizar os jobs. Este arquivo define qual notebook executar, em qual cluster, e seu respectivo agendamento (schedule).
-   **Benef√≠cios:** Esta abordagem de Infraestrutura como C√≥digo (IaC) garante que o ambiente de produ√ß√£o seja um reflexo fiel do que est√° versionado no Git, eliminando configura√ß√µes manuais, reduzindo erros e acelerando o ciclo de desenvolvimento.

## ü§ñ Camada de Consumo: Chatbot & API
A camada Gold, com seus dados de alta qualidade, serve como a base para a interface do usu√°rio.

-   **Databricks Genie:** Atua como uma camada de tradu√ß√£o de linguagem natural para SQL. Ele √© configurado para consultar apenas as tabelas da camada Gold.
-   **n8n:** Orquestra o fluxo da API, conectando o Telegram ao Databricks Genie, atuando como um intermedi√°rio leve entre o usu√°rio e o poder de processamento do Databricks.

## üöÄ Como Executar o Projeto
1.  **Setup do Ambiente:** Configure o workspace no Databricks (clusters, SQL Warehouse).
2.  **Configurar CI/CD:** Adicione os `secrets` necess√°rios (como o token de acesso ao Databricks e o host) nas configura√ß√µes do reposit√≥rio do GitHub para que o GitHub Actions possa se autenticar.
3.  **Definir os Jobs:** Edite o arquivo de configura√ß√£o `jobs.json` (ou nome similar) no reposit√≥rio para definir os notebooks a serem executados e seus respectivos agendamentos no formato `cron`.
4.  **Deploy:** Fa√ßa um push para a branch `main`. O GitHub Actions ser√° acionado e ir√° automaticamente criar ou atualizar os jobs no Databricks com o agendamento definido no JSON.
5.  **Configurar Camada de Consumo:** Configure o Databricks Genie para usar as tabelas da camada Gold e, em seguida, configure o workflow no n8n para conectar o Telegram ao Genie.

## Resumo das Tecnologias
-   **Core do Pipeline:** Apache Spark (Structured Streaming e Batch)
-   **Armazenamento e Lakehouse:** Delta Lake
-   **Automa√ß√£o de CI/CD (IaC):** GitHub Actions
-   **Orquestra√ß√£o do Pipeline:** Databricks Jobs (gerenciados via API/JSON)
-   **Camada de IA/API:** Databricks Genie
-   **Interface do Usu√°rio:** Telegram
-   **Orquestra√ß√£o da API:** n8n
