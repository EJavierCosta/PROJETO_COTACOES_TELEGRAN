{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe892c8d-b131-403f-8491-6366463a811d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "IMPORTS"
    }
   },
   "outputs": [],
   "source": [
    "import delta\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5d9700-f5b7-487e-92a3-e8a2025c2d12",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SETUP"
    }
   },
   "outputs": [],
   "source": [
    "tabela = dbutils.widgets.get(\"tabela\")\n",
    "#tabela =\"cotacoes\"\n",
    "schema = \"cotacoes_bronze\"\n",
    "catalog = \"prod\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc5a69d-9532-409e-9ebf-58668933fe67",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "INGESTÃO FULL LOAD"
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(f\"{catalog}.{schema}.{tabela}\"):\n",
    "\n",
    "        print(\"Tabela não existente, criando.\")\n",
    "\n",
    "        df_full = spark.read.format(\"delta\").table(f\"transacional.cotacoes_db.{tabela}\")\n",
    "\n",
    "        (df_full.coalesce(1)\n",
    "                .write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"overwrite\")\n",
    "                .option(\"delta.enableChangeDataFeed\", \"true\")\n",
    "                .saveAsTable(f\"{catalog}.{schema}.{tabela}\"))\n",
    "else:\n",
    "        print(\"Tabela já existente, ignorando FULL LOAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5900bfcd-d7cc-4879-a417-db5288f8dcb1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LEITURA E ESCRITA CDC COM STREAM"
    }
   },
   "outputs": [],
   "source": [
    "bronze = delta.DeltaTable.forName(spark, f\"{catalog}.{schema}.{tabela}\")\n",
    "\n",
    "def upsert (df, delta):\n",
    "\n",
    "      # Filtra os tipos de mudança que não interessam\n",
    "      df_filtered = df.filter(col(\"_change_type\") != 'update_preimage')\n",
    "\n",
    "      # Define a \"janela\" para encontrar o registro mais recente por Símbolo\n",
    "      windowSpec = Window.partitionBy(\"Simbolo\").orderBy(desc(\"_commit_timestamp\"), desc(\"_commit_version\"))\n",
    "\n",
    "      # Aplica a lógica de dedetização (pega a última atualização para cada Símbolo)\n",
    "      df_cotacoes_cdc = (df_filtered.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "                                    .filter(col(\"row_num\") == 1)\n",
    "                                    .drop(\"row_num\"))\n",
    "\n",
    "      # df.createOrReplaceGlobalTempView(f\"{tabela}_cdc\")\n",
    "      # query = f'''\n",
    "      #       SELECT * \n",
    "      #       FROM global_temp.{tabela}_cdc\n",
    "      #       WHERE _change_type <> 'update_preimage'\n",
    "      #       QUALIFY ROW_NUMBER() OVER (PARTITION BY Simbolo ORDER BY _commit_timestamp DESC, _commit_version DESC) = 1;\n",
    "      #       '''\n",
    "\n",
    "      # df_cotacoes_cdc = spark.sql(query)\n",
    "\n",
    "      (delta.alias(\"b\")\n",
    "        .merge(df_cotacoes_cdc.alias(\"c\"),\"b.Simbolo = c.Simbolo\")\n",
    "        .whenMatchedDelete(condition = \"c._change_type = 'delete'\")\n",
    "        .whenMatchedUpdateAll(condition = \"c._change_type = 'update_postimage'\")\n",
    "        .whenNotMatchedInsertAll(condition = \"c._change_type = 'insert' OR c._change_type = 'update_postimage'\")  \n",
    "        .execute()\n",
    "      )\n",
    "tabela_cdc= (spark.readStream\n",
    "                  .format(\"delta\")\n",
    "                  .option(\"readChangeFeed\", \"true\")\n",
    "                  .option(\"startingVersion\", 0)  # começa do início\n",
    "                  .table(f\"transacional.cotacoes_db.{tabela}\")\n",
    "                  )\n",
    "\n",
    "checkpointpath = f\"/Volumes/transacional/cotacoes_db/checkpoint/checkpoint_{tabela}\"\n",
    "stream = (tabela_cdc.writeStream\n",
    "                    .trigger(availableNow=True)\n",
    "                    .option(\"checkpointLocation\", checkpointpath)\n",
    "                    .foreachBatch(lambda df, batchId: upsert(df, bronze))\n",
    "                    )\n",
    "\n",
    "start = stream.start()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7901972089961355,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
